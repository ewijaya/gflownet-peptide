# GFlowNet Peptide Generation - Default Configuration
# Phase 2: GFlowNet Core Implementation
#
# References:
#   - Bengio et al. (2021): Flow Network based Generative Models
#   - Malkin et al. (2022): Trajectory Balance: Improved Credit Assignment in GFlowNets
#   - Jain et al. (2022): Biological Sequence Design with GFlowNets

# =============================================================================
# Forward Policy (P_F)
# =============================================================================
policy:
  # Transformer architecture
  vocab_size: 23  # 20 amino acids + START (20) + STOP (21) + PAD (22)
  d_model: 256
  n_layers: 4
  n_heads: 8
  dim_feedforward: 1024  # 4 * d_model per PRD
  dropout: 0.1
  max_length: 64

  # Positional encoding
  pos_encoding: "sinusoidal"  # or "learned"

# =============================================================================
# Backward Policy (P_B)
# =============================================================================
backward_policy:
  type: "uniform"  # For linear autoregressive generation, P_B = 1
  # type: "learned"  # Alternative for more complex state spaces

# =============================================================================
# Reward Model
# =============================================================================
reward:
  # ESM-2 backbone
  esm_model: "esm2_t33_650M_UR50D"  # or "esm2_t12_35M_UR50D" for faster training
  freeze_esm: true

  # Reward heads
  stability_head:
    hidden_dim: 256
    n_layers: 2
    activation: "relu"

  binding_head:
    hidden_dim: 256
    n_layers: 2
    activation: "relu"

  # Composite reward weights
  weights:
    stability: 1.0
    binding: 1.0
    naturalness: 0.5

  # Non-negativity transform
  transform: "exp"  # or "softplus"

  # Temperature for P(x) ∝ R(x)^β
  temperature: 1.0

# =============================================================================
# Training
# =============================================================================
training:
  # Optimization
  learning_rate: 3.0e-4
  weight_decay: 0.01
  lr_scheduler: "cosine"
  warmup_steps: 1000

  # log_Z learning rate multiplier (Malkin 2022)
  # CHANGED from 10.0 to 3.0 for stability
  # See docs/reward-comparison-analysis.md Section 3.4
  log_z_lr_multiplier: 3.0

  # Batch and steps
  batch_size: 64
  n_steps: 100000
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0

  # Loss function
  # CHANGED from "trajectory_balance" to "sub_trajectory_balance"
  # STB provides stable loss curves suitable for publication
  # See docs/prd-phase-3b-training-stability.md Section 2.5
  loss_type: "sub_trajectory_balance"

  # Sub-trajectory balance decay factor (λ in the paper)
  # Controls contribution weighting: L_t = λ^(T-t) × residual²
  # Higher values (0.9-0.99) give more weight to later steps
  lambda_sub: 0.9

  # Entropy regularization weight
  # Encourages exploration, prevents mode collapse
  # L_total = L_STB - entropy_weight * mean(log_P_F)
  # Recommended: 0.01-0.1, set to 0.0 to disable
  entropy_weight: 0.01

  # Checkpointing (policy: overwrite latest, keep final)
  save_every: 5000
  eval_every: 1000
  log_every: 100

  # Learnable log Z (partition function)
  init_log_z: 0.0

  # Reproducibility
  seed: 42

# =============================================================================
# Data
# =============================================================================
data:
  # FLIP dataset
  flip_path: "data/flip/"
  flip_tasks:
    - "stability"
    - "gb1"

  # Propedia dataset
  propedia_path: "data/propedia/"

  # Preprocessing
  min_length: 10
  max_length: 50
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

# =============================================================================
# Generation / Sampling
# =============================================================================
generation:
  min_length: 10
  max_length: 30

  # Sampling parameters
  sample_temperature: 1.0  # Higher = more uniform, lower = more peaked
  top_k: 0  # 0 = disabled
  top_p: 1.0  # 1.0 = disabled

  # Exploration (Bengio 2021, Eq. 10)
  # π_explore = (1 - δ) * P_F + δ * Uniform
  exploration_eps: 0.001  # δ: uniform mixing coefficient

  # Batch generation
  batch_size: 128

# =============================================================================
# Evaluation
# =============================================================================
evaluation:
  n_samples: 1000

  # Diversity metrics
  compute_sequence_diversity: true
  compute_embedding_diversity: true
  compute_clusters: true

  # Clustering parameters
  umap_n_neighbors: 15
  umap_min_dist: 0.1
  hdbscan_min_cluster_size: 5

  # Proportionality check
  n_reward_bins: 10

# =============================================================================
# Logging
# =============================================================================
logging:
  use_wandb: true
  wandb_project: "gflownet-peptide"
  wandb_entity: "ewijaya"  # W&B username

  log_every: 100

  # Console output
  verbose: true

# =============================================================================
# Hardware
# =============================================================================
hardware:
  device: "cuda"  # or "cpu"
  num_workers: 4
  pin_memory: true

  # Mixed precision
  use_amp: true
  amp_dtype: "float16"  # or "bfloat16"

# =============================================================================
# Paths
# =============================================================================
paths:
  output_dir: "outputs/"
  checkpoint_dir: "checkpoints/"
  sample_dir: "samples/"
  log_dir: "logs/"
