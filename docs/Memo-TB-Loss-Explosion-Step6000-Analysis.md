# Memo: Why TB Loss Explodes Around Step 6000

**Date**: 2025-12-28
**Context**: Analysis of GFlowNet training runs showing loss explosion pattern

## Summary

All GFlowNet training runs show loss explosion around step 5000-7000. This memo explains why this happens and what factors determine the timing.

---

## Part 1: What Happens at Step 6000?

The data reveals a **phase transition** â€” the policy discovers high-reward modes:

### Example: `reward-C-improved` Run

| Step | Loss | mean_log_pf | mean_reward | What's Happening |
|------|------|-------------|-------------|------------------|
| 5000 | 32 | -20.6 | 0.06 | Policy exploring, low reward |
| 6000 | 21 | -20.7 | 0.06 | Still exploring, loss decreasing âœ“ |
| 7000 | **1327** | **-51.3** | **0.79** | ðŸ”¥ Policy found high-reward mode |
| 8000 | 2362 | -64.3 | 0.93 | Policy becoming confident on that mode |

### The Mechanism

```
Step 1-5000: Policy explores randomly
             â†’ log_pf â‰ˆ -20 (uncertain)
             â†’ reward â‰ˆ 0.06 (low)
             â†’ loss decreasing (learning)

Step 6000+:  Policy discovers high-reward sequences
             â†’ reward jumps 0.06 â†’ 0.79 â†’ 0.93
             â†’ Policy becomes CONFIDENT on these
             â†’ log_pf drops -20 â†’ -51 â†’ -64 (very confident)
             â†’ log_Z can't keep up
             â†’ Loss explodes
```

### The Formula

The TB loss is:
```
L = (log_Z + log_P_F - log_R)Â²
```

When policy finds good sequences:
- `log_R` increases (better reward)
- `log_P_F` becomes very negative (confident policy)
- `log_Z` learns slowly (learning rate limited)

**The imbalance grows â†’ squared error explodes**

### Explosion Timing Varies by Run

| Run | Explosion Step | Trigger |
|-----|----------------|---------|
| baseline | ~5000 | reward stays flat (0.44) but log_pf drops |
| reward-C-improved | ~6000-7000 | reward jumps 0.06 â†’ 0.79 |
| reward-A-trained | ~2000 (earlier!) | reward climbs steadily from start |
| reward-B-esm2pll | Never | Collapsed immediately (all same output) |
| phase3b-stb | ~5000-7000 | reward jumps 0.18 â†’ 0.82 |
| phase3b-entropy-0.00 | ~4000 (earlier!) | reward jumps 0.48 â†’ 0.91 |

### Key Insight

**The explosion is correlated with reward discovery, not a fixed step count.**

The ~6000 step convergence across runs is because:
1. Similar policy architecture (same capacity)
2. Similar learning rate (same optimization speed)
3. Similar reward landscape difficulty

**Bottom line**: Step 6000 is when the policy "gets it" â€” finds the high-reward region and commits to it. The loss explosion is the **signature of successful learning**, not failure.

---

## Part 2: Why Step ~6000 Specifically?

### Training Configuration

| Parameter | Value |
|-----------|-------|
| Learning rate | 3e-4 |
| Batch size | 64 |
| Warmup steps | 1000 |
| Policy size | 4 layers, 256 dim, 8 heads |
| Sequence length | 10-30 amino acids |
| Vocab size | 21 actions (20 AA + STOP) |

### By Step 6000, You've Done:

```
Total gradient updates:     6,000
Total sequences sampled:    6,000 Ã— 64 = 384,000
Effective training steps:   5,000 (after 1000 warmup)
Cumulative LR Ã— steps:      5,000 Ã— 3e-4 = 1.5 (rough measure of "learning done")
```

### Why NOT Step 500?

At step 500:
- Only 32,000 sequences sampled
- Still in warmup (LR ramping up)
- Not enough gradient updates to shift the policy distribution significantly
- Policy is essentially still random

### Why NOT Step 9000?

By step 9000:
- The explosion already happened at step 6000
- It's a **one-time phase transition**, not something that keeps happening

### Factors That Determine Explosion Timing

| Factor | Effect on Timing |
|--------|------------------|
| **1. Search space size** | 21^20 â‰ˆ 10^26 possible sequences â€” need many samples to find patterns |
| **2. Reward sparsity** | How many sequences have high reward? Sparser = takes longer |
| **3. Learning rate** | 3e-4 is moderate â€” faster LR = earlier discovery |
| **4. Model capacity** | 4-layer Transformer can capture patterns after ~5k updates |
| **5. Batch size** | 64 samples/step provides stable gradients |

### Evidence: It's NOT Always Step 6000

| Run | Explosion Step | Why Different? |
|-----|----------------|----------------|
| `entropy-0.00` | ~4000 | No entropy penalty â†’ faster mode collapse |
| `reward-A-trained` | ~2000 | Reward signal stronger â†’ faster discovery |
| `reward-C-improved` | ~7000 | More exploration â†’ slower discovery |
| `reward-B-esm2pll` | Never | Degenerate reward â†’ immediate collapse |

### The Formula (Intuitive)

```
Explosion Step â‰ˆ f(search_difficulty, learning_rate, model_capacity, exploration)
```

For this setup:
```
~6000 â‰ˆ f(21^20 space, 3e-4 LR, 4-layer Transformer, 0.01 entropy)
```

### What Would Change the Timing?

| Change | Effect |
|--------|--------|
| Higher learning rate (1e-3) | Earlier explosion (~2000 steps) |
| Lower learning rate (1e-4) | Later explosion (~15000 steps) |
| More entropy regularization (0.1) | Later explosion |
| Less entropy (0.0) | Earlier explosion (confirmed in runs) |
| Larger model (8 layers) | Possibly earlier (more capacity) |
| Smaller batch (16) | Possibly later (noisier gradients) |

---

## Conclusion

**Step 6000 is not a fundamental constant** â€” it's an emergent property of:
- Hyperparameters (LR, batch size, warmup)
- Model architecture (4-layer Transformer)
- Reward landscape (peptide fitness)
- Exploration settings (entropy weight)

Change any of these, and the explosion moves earlier or later. The ~6000 step convergence across runs happens because they share most of these settings.

---

## Analyzed Runs

| ID | Name | State | Created |
|----|------|-------|---------|
| 8rflp7l6 | gflownet-baseline-10k | finished | 2025-12-26 |
| zcb95gyl | gflownet-reward-C-improved-10k | finished | 2025-12-26 |
| 3fr3yzn0 | gflownet-reward-B-esm2pll-10k | finished | 2025-12-26 |
| 6qsqq6wz | gflownet-reward-A-trained-10k | finished | 2025-12-26 |
| nodeps9c | phase3b-stb-validation-10k | finished | 2025-12-27 |
| zv30wmny | phase3b-entropy-0.00-10k | running | 2025-12-27 |
