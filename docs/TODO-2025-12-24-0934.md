# GFlowNet Peptide - Session TODO

**Created**: 2025-12-24 09:34 JST
**Last Updated**: 2025-12-24 09:34 JST
**Previous TODO**: [TODO-2025-12-23-1630.md](./TODO-2025-12-23-1630.md)

---

## Current Phase: Phase 0b - Improved Reward Validation

**Objective**: Fix ESM-2 pseudo-likelihood reward hacking by implementing an entropy-gated reward function, then re-train GRPO-D to determine if the diversity problem persists.

**Key Question**: Is the diversity problem caused by the reward function (ESM-2 PLL) or the optimization method (GRPO-D)?

---

## In Progress

- [ ] **Phase 0b Training: GRPO-D with Improved Reward** (started 2025-12-24 09:33 UTC)
  - Monitor: https://wandb.ai/ewijaya/gflownet-peptide/runs/udhmfpso
  - Logs: `tail -f logs/train_grpo_improved.log`
  - Config: `configs/grpo_improved.yaml`
  - Expected duration: ~9-10 hours
  - Auto-shutdown: Instance will stop when complete (`/home/ubuntu/bin/stopinstance`)

---

## Completed This Session

### Code Development

- **Created `gflownet_peptide/rewards/improved_reward.py`**: New `ImprovedReward` class with 3 components:
  - Embedding naturalness (ESM-2 embedding norm)
  - Entropy gate (penalizes low-complexity/repetitive sequences)
  - Length gate (penalizes too-short sequences)
- **Created `scripts/validate_reward.py`**: Validation script with test cases for real peptides, homopolymers, and edge cases
- **Updated `scripts/train_grpo.py`**: Added `--reward_type` flag to switch between `esm2_pll` and `improved`
- **Created `configs/grpo_improved.yaml`**: New config for Phase 0b training with improved reward parameters
- **Updated `gflownet_peptide/rewards/__init__.py`**: Added `ImprovedReward` export

### Documentation

- **Created `docs/prd-phase-0b-improved-reward.md`**: Comprehensive PRD for Phase 0b implementation
- **Created `docs/reward-design-analysis_2025-12-24.md`**: Three-perspective analysis (Bengio, Sutton, Ng) for reward design
- **Updated `docs/reward_formulation.md`**: Added Section 3 for improved reward mathematical specification
- **Updated `docs/gflownet-master-prd.md`**:
  - Added Phase 0a/0b subdivision
  - Added ESM-2 reward hacking findings
  - Added risk R2a (confirmed reward hacking)
  - Updated to version 1.1
- **Created `.claude/commands/reward-design.md`**: Slash command for three-perspective reward analysis

### Evaluation & Analysis

- **Ran reward validation**: All checks passed
  - R(real) > R(repetitive) for 100% of test pairs ✅
  - R(homopolymer) < 0.1 for all homopolymers ✅
  - R(real) > 0.5 for all real peptides ✅

---

## Next Steps

After training completes (~9-10 hours):

- [ ] Check training completed successfully (logs, W&B)
- [ ] Load generated peptides from `results/grpo/<run>_peptides.csv`
- [ ] Compute Phase 0b metrics:
  - [ ] Repeat rate (target: <20%)
  - [ ] Mean sequence entropy (target: >0.6)
  - [ ] Cluster count via HDBSCAN (target: >10)
  - [ ] Embedding diversity (target: >0.4)
- [ ] Create comparison analysis (Phase 0a vs 0b)
- [ ] Update `docs/phase0_decision.md` with Phase 0b findings
- [ ] Make final GO/NO-GO decision for GFlowNet

---

## Observations

### Phase 0a Findings (ESM-2 Pseudo-Likelihood)

- **97% of generated sequences contained repetitive patterns** (e.g., `QQQQQQQQQQ`, `NNNNNNNNNN`)
- ESM-2 pseudo-likelihood rewards predictability, not biological viability
- Top peptides like `MRQQQQQQQQQQQQQQQQNNNNNNNNNNNN` scored ~0.93
- This is **reward hacking**, not a GRPO-D limitation

### Improved Reward Design

- **Key insight**: Replace "is each AA predictable?" (pseudo-likelihood) with "does this look like a real protein?" (embedding norm) + "is this low-complexity?" (entropy gate)
- Entropy gate uses Shannon entropy of AA distribution, normalized to [0,1]
- Soft sigmoid gates allow gradients while effectively zeroing reward for degenerate sequences
- Multiplicative combination ensures ALL components must be good

### Early Training Observation

- Initial iteration shows some repetitive patterns still scoring high (e.g., `HISPVSPVSPVSPVSPVS...`)
- This is expected early in training; the entropy gate should penalize these over time
- Mean reward starting at 0.316 (lower than Phase 0a's 0.816, as expected)

---

## Files Modified

| File | Action |
|------|--------|
| `gflownet_peptide/rewards/improved_reward.py` | Created |
| `gflownet_peptide/rewards/__init__.py` | Modified |
| `scripts/validate_reward.py` | Created |
| `scripts/train_grpo.py` | Modified |
| `configs/grpo_improved.yaml` | Created |
| `docs/prd-phase-0b-improved-reward.md` | Created |
| `docs/reward-design-analysis_2025-12-24.md` | Created |
| `docs/reward_formulation.md` | Modified |
| `docs/gflownet-master-prd.md` | Modified |
| `docs/phase0_decision.md` | Modified |
| `.claude/commands/reward-design.md` | Created |
| `notebooks/gflownet-phase-0-validation.ipynb` | Modified |

---

## Commands Reference

```bash
# Monitor training
tail -f logs/train_grpo_improved.log

# Check if training is running
ps aux | grep train_grpo

# W&B dashboard
# https://wandb.ai/ewijaya/gflownet-peptide/runs/udhmfpso
```

---

*End of TODO*
