# GFlowNet Peptide - Session TODO

**Created**: 2025-12-26 16:00 JST
**Last Updated**: 2025-12-26 16:00 JST
**Previous TODO**: [TODO-2025-12-24-2333.md](./TODO-2025-12-24-2333.md)

---

## Current Phase: Phase 3 - Training & Hyperparameter Tuning

**Objective**: Train the GFlowNet model to convergence using Trajectory Balance loss, tune hyperparameters for optimal diversity-quality trade-off, and generate a large peptide dataset for downstream evaluation.

**PRD**: [docs/prd-phase-3-training.md](./prd-phase-3-training.md)

---

## Completed This Session

### Code Development

- Fixed `ESMBackbone` device handling for lazy loading (override `_apply()` method)
- Fixed `RewardHead` sigmoid transform (was returning raw values)
- Generated Phase 3 PRD from master PRD

### Training

- ✅ Sanity check (100 steps) completed successfully
- W&B run: https://wandb.ai/ewijaya/gflownet-peptide/runs/5ej9lfs9

### Tests

- All 123 tests passing
- Phase 2 coverage: 87%

---

## Phase 3 Command Execution Checklist

### Step 0: Environment Setup (Already Done)

```bash
# Verify GPU availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"

# Verify W&B login
wandb login --verify

# Create logs directory
mkdir -p logs checkpoints/gflownet samples outputs
```

- [x] GPU verified
- [x] W&B logged in
- [x] Directories created

---

### Step 1: Sanity Check (100 steps) ✅ COMPLETED

```bash
python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 100 \
  --batch_size 32 \
  --output_dir checkpoints/gflownet/sanity_check/ \
  --wandb
```

- [x] Training completed without errors
- [x] W&B metrics logged

---

### Step 2: Baseline Training (10K steps)

**Duration**: ~30-60 minutes

**Option A: Interactive (if staying connected)**
```bash
python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 10000 \
  --output_dir checkpoints/gflownet/baseline/ \
  --wandb
```

**Option B: Background with auto-shutdown (recommended)**
```bash
nohup bash -c 'python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 10000 \
  --output_dir checkpoints/gflownet/baseline/ \
  --run_name gflownet-baseline-10k \
  --wandb \
  > logs/train_baseline.log 2>&1 && /home/ubuntu/bin/stopinstance' &
```

**Monitoring**:
```bash
# Check progress
tail -f logs/train_baseline.log

# Check if running
ps aux | grep train_gflownet

# View W&B dashboard
# https://wandb.ai/ewijaya/gflownet-peptide
```

**Verification after completion**:
```bash
ls -la checkpoints/gflownet/baseline/

python -c "
import torch
ckpt = torch.load('checkpoints/gflownet/baseline/gflownet_final.pt')
print('Checkpoint keys:', ckpt.keys())
print('Step:', ckpt.get('step', 'N/A'))
"
```

- [ ] Training completed
- [ ] Checkpoint saved
- [ ] Metrics look reasonable (loss decreasing, no NaN)

---

### Step 3: Analyze Baseline Results

```bash
# Check W&B for metrics
python -c "
import wandb
api = wandb.Api()
runs = api.runs('ewijaya/gflownet-peptide')
for run in runs[:5]:
    print(f'{run.name}: {run.state}, loss={run.summary.get(\"train/loss\", \"N/A\")}')"
```

- [ ] Loss converged or trending down
- [ ] log_Z stable (not diverging)
- [ ] No NaN values

---

### Step 4: Hyperparameter Sweep (Optional - Skip if baseline is good)

**Option A: W&B Sweep**
```bash
# Create sweep config
cat > configs/sweep.yaml << 'EOF'
program: scripts/train_gflownet.py
method: bayes
metric:
  name: train/loss
  goal: minimize
parameters:
  learning_rate:
    values: [1e-4, 3e-4, 1e-3]
  batch_size:
    values: [32, 64, 128]
EOF

# Create sweep
wandb sweep configs/sweep.yaml --project gflownet-peptide

# Run agent (replace <sweep_id> with actual ID)
# wandb agent ewijaya/gflownet-peptide/<sweep_id>
```

**Option B: Manual Grid Search**
```bash
# Learning rate sweep
for lr in 1e-4 3e-4 1e-3; do
  nohup python scripts/train_gflownet.py \
    --config configs/default.yaml \
    --n_steps 10000 \
    --output_dir checkpoints/gflownet/sweep_lr_${lr}/ \
    --wandb \
    > logs/sweep_lr_${lr}.log 2>&1 &
done
```

- [ ] Sweep completed (if running)
- [ ] Best hyperparameters identified
- [ ] `configs/best.yaml` created (if different from default)

---

### Step 5: Full Training (100K steps)

**Duration**: ~4-8 hours

```bash
nohup bash -c 'python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 100000 \
  --output_dir checkpoints/gflownet/final/ \
  --run_name gflownet-final-100k \
  --wandb \
  > logs/train_final.log 2>&1 && /home/ubuntu/bin/stopinstance' &
```

**Monitoring (from another session)**:
```bash
tail -f logs/train_final.log
```

**Verification after completion**:
```bash
ls -la checkpoints/gflownet/final/

python -c "
import torch
ckpt = torch.load('checkpoints/gflownet/final/gflownet_final.pt')
print('Step:', ckpt.get('step'))
print('Final log_Z:', ckpt.get('log_z'))
"
```

- [ ] Training completed to 100K steps
- [ ] Final checkpoint saved
- [ ] Loss converged
- [ ] log_Z stable

---

### Step 6: Generate Peptide Samples

```bash
python scripts/sample.py \
  --checkpoint checkpoints/gflownet/final/gflownet_final.pt \
  --n_samples 10000 \
  --output samples/gflownet_final.csv \
  --temperature 1.0
```

**Quick validation**:
```bash
python -c "
import pandas as pd
df = pd.read_csv('samples/gflownet_final.csv')
print(f'Total samples: {len(df)}')
print(f'Unique samples: {df[\"sequence\"].nunique()}')
print(f'Mean length: {df[\"sequence\"].str.len().mean():.1f}')
print(f'Mean reward: {df[\"reward\"].mean():.4f}')
"
```

- [ ] 10,000 samples generated
- [ ] Samples saved to CSV
- [ ] Basic stats look reasonable

---

### Step 7: Evaluate Samples

```bash
python scripts/evaluate.py \
  --gflownet_samples samples/gflownet_final.csv \
  --output outputs/phase3_eval.json
```

**View results**:
```bash
cat outputs/phase3_eval.json | python -m json.tool
```

- [ ] Evaluation completed
- [ ] Diversity metrics computed
- [ ] Quality metrics computed

---

### Step 8: Phase Gate Review

**Success Criteria Verification**:

| Criterion | Target | Actual | Status |
|-----------|--------|--------|--------|
| Training loss | Converged | ______ | [ ] |
| log_Z stability | No divergence | ______ | [ ] |
| Sample validity | ≥99% valid AA | ______ | [ ] |
| Sample diversity | > 0.5 | ______ | [ ] |
| Sample quality | Mean R > 0.5 | ______ | [ ] |
| Generated samples | 10,000 | ______ | [ ] |

**Final verification commands**:
```bash
# Check all outputs exist
ls -la checkpoints/gflownet/final/gflownet_final.pt
ls -la samples/gflownet_final.csv
ls -la outputs/phase3_eval.json
wc -l samples/gflownet_final.csv
```

- [ ] All success criteria met
- [ ] Ready for Phase 4 (Evaluation & GRPO Comparison)

---

## Quick Reference: All Commands in Order

```bash
# === STEP 1: Sanity Check (DONE) ===
python scripts/train_gflownet.py --config configs/default.yaml --n_steps 100 --batch_size 32 --output_dir checkpoints/gflownet/sanity_check/ --wandb

# === STEP 2: Baseline Training ===
mkdir -p logs
nohup bash -c 'python scripts/train_gflownet.py --config configs/default.yaml --n_steps 10000 --output_dir checkpoints/gflownet/baseline/ --run_name gflownet-baseline-10k --wandb > logs/train_baseline.log 2>&1 && /home/ubuntu/bin/stopinstance' &

# === STEP 5: Full Training (after baseline looks good) ===
nohup bash -c 'python scripts/train_gflownet.py --config configs/default.yaml --n_steps 100000 --output_dir checkpoints/gflownet/final/ --run_name gflownet-final-100k --wandb > logs/train_final.log 2>&1 && /home/ubuntu/bin/stopinstance' &

# === STEP 6: Generate Samples ===
python scripts/sample.py --checkpoint checkpoints/gflownet/final/gflownet_final.pt --n_samples 10000 --output samples/gflownet_final.csv --temperature 1.0

# === STEP 7: Evaluate ===
python scripts/evaluate.py --gflownet_samples samples/gflownet_final.csv --output outputs/phase3_eval.json
```

---

## Observations

- ESM-2 model (esm2_t33_650M) downloads on first use (~2.5GB)
- Training uses ~8GB GPU memory with batch_size=32
- Sanity check showed loss dropping from ~1049 to ~0.78 in 100 steps
- log_Z learning is working (started at 0, moved to 0.08)
- CompositeReward is functional after sigmoid fix

---

## Files Modified This Session

| File | Action |
|------|--------|
| `gflownet_peptide/models/reward_model.py` | Fixed ESMBackbone device + sigmoid |
| `docs/prd-phase-3-training.md` | Created + updated |
| `docs/TODO-2025-12-26-1600.md` | Created |

---

## W&B Dashboard

- Project: https://wandb.ai/ewijaya/gflownet-peptide
- Sanity check run: https://wandb.ai/ewijaya/gflownet-peptide/runs/5ej9lfs9
