# GFlowNet Peptide - Session TODO

**Created**: 2025-12-26 16:00 JST
**Last Updated**: 2025-12-26 21:00 JST
**Previous TODO**: [TODO-2025-12-24-2333.md](./TODO-2025-12-24-2333.md)

---

## Current Phase: Phase 3 - Training & Hyperparameter Tuning

**Objective**: Train the GFlowNet model to convergence using Trajectory Balance loss, tune hyperparameters for optimal diversity-quality trade-off, and generate a large peptide dataset for downstream evaluation.

**PRD**: [docs/prd-phase-3-training.md](./prd-phase-3-training.md)

---

## Completed This Session

### Code Development

- Fixed `ESMBackbone` device handling for lazy loading (override `_apply()` method)
- Fixed `RewardHead` sigmoid transform (was returning raw values)
- Generated Phase 3 PRD from master PRD
- **Added `--reward_type` flag** to `scripts/train_gflownet.py` supporting:
  - `composite` (legacy, untrained MLP heads)
  - `improved` (Option C: entropy gate + naturalness)
  - `esm2_pll` (Option B: ESM-2 pseudo-likelihood)
  - `trained` (Option A: trained stability predictor)
- Updated documentation: `reward_formulation.md`, `gflownet-master-prd.md`, `prd-phase-3-training.md`

### Training

- âœ… Sanity check (100 steps) completed successfully
- W&B run: https://wandb.ai/ewijaya/gflownet-peptide/runs/5ej9lfs9

### Tests

- All 123 tests passing
- Phase 2 coverage: 87%

---

## Phase 3 Command Execution Checklist

### Step 0: Environment Setup (Already Done)

```bash
# Verify GPU availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"

# Verify W&B login
wandb login --verify

# Create logs directory
mkdir -p logs checkpoints/gflownet samples outputs
```

- [x] GPU verified
- [x] W&B logged in
- [x] Directories created

---

### Step 1: Sanity Check (100 steps) âœ… COMPLETED

```bash
python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 100 \
  --batch_size 32 \
  --output_dir checkpoints/gflownet/sanity_check/ \
  --wandb
```

- [x] Training completed without errors
- [x] W&B metrics logged

---

### Step 2: Baseline Training (10K steps) âœ… COMPLETED

**W&B Run**: https://wandb.ai/ewijaya/gflownet-peptide/runs/8rflp7l6

- [x] Training completed (10,000 steps in ~2h 51m)
- [x] Checkpoints saved (gflownet_latest.pt, gflownet_best.pt, gflownet_final.pt)

**Final Results**:

| Metric | Value | Status |
|--------|-------|--------|
| Diversity | 0.92 | âœ… Excellent |
| Mean Reward | 0.445 | âš ï¸ Flat (see root cause below) |
| Loss | 4079 | âš ï¸ Exploded after step 5000 |
| log_Z | 17.7 | Diverged |

**Root Cause Analysis**: The baseline used untrained MLP heads (`CompositeReward` from `models/reward_model.py`) which produced near-constant rewards (~0.48-0.52). Log reward range was only 0.03 units vs policy log_PF of ~82 units - the reward signal was completely drowned out.

**Solution**: Implemented `--reward_type` flag with three discriminative reward options (see Step 2b below)

---

### Step 2b: Reward Comparison Runs (A, B, C) ðŸ”„ IN PROGRESS

**Objective**: Compare three reward functions to find one with sufficient dynamic range for GFlowNet learning.

| Option | `--reward_type` | Description | W&B Run Name |
|--------|-----------------|-------------|--------------|
| **C** | `improved` | Entropy gate + naturalness | `gflownet-reward-C-improved-10k` |
| **B** | `esm2_pll` | ESM-2 pseudo-likelihood | `gflownet-reward-B-esm2pll-10k` |
| **A** | `trained` | Trained stability predictor | `gflownet-reward-A-trained-10k` |

**Sequential Execution Command**:
```bash
nohup bash -c '
echo "=== Starting reward comparison runs ===" && \
python scripts/train_gflownet.py --reward_type improved --esm_model esm2_t6_8M_UR50D --n_steps 10000 --output_dir checkpoints/gflownet/reward-comparison/improved/ --run_name gflownet-reward-C-improved-10k --wandb --seed 42 && \
python scripts/train_gflownet.py --reward_type esm2_pll --esm_model esm2_t6_8M_UR50D --n_steps 10000 --output_dir checkpoints/gflownet/reward-comparison/esm2pll/ --run_name gflownet-reward-B-esm2pll-10k --wandb --seed 42 && \
python scripts/train_gflownet.py --reward_type trained --reward_checkpoint checkpoints/reward_models/stability_predictor_best.pt --esm_model esm2_t6_8M_UR50D --n_steps 10000 --output_dir checkpoints/gflownet/reward-comparison/trained/ --run_name gflownet-reward-A-trained-10k --wandb --seed 42 && \
/home/ubuntu/bin/stopinstance
' > logs/reward-comparison-$(date +%Y%m%d_%H%M%S).log 2>&1 &
```

**Current Status**:
- ðŸ”„ **Run 1/3**: `gflownet-reward-C-improved-10k` - IN PROGRESS
- â³ **Run 2/3**: `gflownet-reward-B-esm2pll-10k` - PENDING
- â³ **Run 3/3**: `gflownet-reward-A-trained-10k` - PENDING

**Expected Runtime**: ~9 hours total (~3 hours per run)

**Success Criteria**:
- [ ] `train/mean_reward` shows increasing trend (not flat like baseline)
- [ ] `train/loss` decreases or stabilizes
- [ ] `eval/sequence_diversity` > 0.6
- [ ] No NaN values

---

### Step 3: Analyze Baseline Results âœ… COMPLETED

**Root Cause Identified**: Untrained MLP heads produced flat rewards (~0.48-0.52)

**Action Taken**: Implemented `--reward_type` flag with three options (A, B, C) - see Step 2b

---

### Step 4: Hyperparameter Sweep (Optional - Skip if baseline is good)

**Option A: W&B Sweep**
```bash
# Create sweep config
cat > configs/sweep.yaml << 'EOF'
program: scripts/train_gflownet.py
method: bayes
metric:
  name: train/loss
  goal: minimize
parameters:
  learning_rate:
    values: [1e-4, 3e-4, 1e-3]
  batch_size:
    values: [32, 64, 128]
EOF

# Create sweep
wandb sweep configs/sweep.yaml --project gflownet-peptide

# Run agent (replace <sweep_id> with actual ID)
# wandb agent ewijaya/gflownet-peptide/<sweep_id>
```

**Option B: Manual Grid Search**
```bash
# Learning rate sweep
for lr in 1e-4 3e-4 1e-3; do
  nohup python scripts/train_gflownet.py \
    --config configs/default.yaml \
    --n_steps 10000 \
    --output_dir checkpoints/gflownet/sweep_lr_${lr}/ \
    --wandb \
    > logs/sweep_lr_${lr}.log 2>&1 &
done
```

- [ ] Sweep completed (if running)
- [ ] Best hyperparameters identified
- [ ] `configs/best.yaml` created (if different from default)

---

### Step 5: Full Training (100K steps)

**Duration**: ~4-8 hours

```bash
nohup bash -c 'python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 100000 \
  --output_dir checkpoints/gflownet/final/ \
  --run_name gflownet-final-100k \
  --wandb \
  > logs/train_final.log 2>&1 && /home/ubuntu/bin/stopinstance' &
```

**Monitoring (from another session)**:
```bash
tail -f logs/train_final.log
```

**Verification after completion**:
```bash
ls -la checkpoints/gflownet/final/

python -c "
import torch
ckpt = torch.load('checkpoints/gflownet/final/gflownet_final.pt')
print('Step:', ckpt.get('step'))
print('Final log_Z:', ckpt.get('log_z'))
"
```

- [ ] Training completed to 100K steps
- [ ] Final checkpoint saved
- [ ] Loss converged
- [ ] log_Z stable

---

### Step 6: Generate Peptide Samples

```bash
python scripts/sample.py \
  --checkpoint checkpoints/gflownet/final/gflownet_final.pt \
  --n_samples 10000 \
  --output samples/gflownet_final.csv \
  --temperature 1.0
```

**Quick validation**:
```bash
python -c "
import pandas as pd
df = pd.read_csv('samples/gflownet_final.csv')
print(f'Total samples: {len(df)}')
print(f'Unique samples: {df[\"sequence\"].nunique()}')
print(f'Mean length: {df[\"sequence\"].str.len().mean():.1f}')
print(f'Mean reward: {df[\"reward\"].mean():.4f}')
"
```

- [ ] 10,000 samples generated
- [ ] Samples saved to CSV
- [ ] Basic stats look reasonable

---

### Step 7: Evaluate Samples

```bash
python scripts/evaluate.py \
  --gflownet_samples samples/gflownet_final.csv \
  --output outputs/phase3_eval.json
```

**View results**:
```bash
cat outputs/phase3_eval.json | python -m json.tool
```

- [ ] Evaluation completed
- [ ] Diversity metrics computed
- [ ] Quality metrics computed

---

### Step 8: Phase Gate Review

**Success Criteria Verification**:

| Criterion | Target | Actual | Status |
|-----------|--------|--------|--------|
| Training loss | Converged | ______ | [ ] |
| log_Z stability | No divergence | ______ | [ ] |
| Sample validity | â‰¥99% valid AA | ______ | [ ] |
| Sample diversity | > 0.5 | ______ | [ ] |
| Sample quality | Mean R > 0.5 | ______ | [ ] |
| Generated samples | 10,000 | ______ | [ ] |

**Final verification commands**:
```bash
# Check all outputs exist
ls -la checkpoints/gflownet/final/gflownet_final.pt
ls -la samples/gflownet_final.csv
ls -la outputs/phase3_eval.json
wc -l samples/gflownet_final.csv
```

- [ ] All success criteria met
- [ ] Ready for Phase 4 (Evaluation & GRPO Comparison)

---

## Quick Reference: All Commands in Order

```bash
# === STEP 1: Sanity Check (DONE) ===
python scripts/train_gflownet.py --config configs/default.yaml --n_steps 100 --batch_size 32 --output_dir checkpoints/gflownet/sanity_check/ --wandb

# === STEP 2: Baseline Training ===
mkdir -p logs
nohup bash -c 'python scripts/train_gflownet.py --config configs/default.yaml --n_steps 10000 --output_dir checkpoints/gflownet/baseline/ --run_name gflownet-baseline-10k --wandb > logs/train_baseline.log 2>&1 && /home/ubuntu/bin/stopinstance' &

# === STEP 5: Full Training (after baseline looks good) ===
nohup bash -c 'python scripts/train_gflownet.py --config configs/default.yaml --n_steps 100000 --output_dir checkpoints/gflownet/final/ --run_name gflownet-final-100k --wandb > logs/train_final.log 2>&1 && /home/ubuntu/bin/stopinstance' &

# === STEP 6: Generate Samples ===
python scripts/sample.py --checkpoint checkpoints/gflownet/final/gflownet_final.pt --n_samples 10000 --output samples/gflownet_final.csv --temperature 1.0

# === STEP 7: Evaluate ===
python scripts/evaluate.py --gflownet_samples samples/gflownet_final.csv --output outputs/phase3_eval.json
```

---

## Observations

- ESM-2 model (esm2_t33_650M) downloads on first use (~2.5GB)
- Training uses ~8GB GPU memory with batch_size=32
- Sanity check showed loss dropping from ~1049 to ~0.78 in 100 steps
- log_Z learning is working (started at 0, moved to 0.08)
- CompositeReward is functional after sigmoid fix

---

## Files Modified This Session

| File | Action |
|------|--------|
| `gflownet_peptide/models/reward_model.py` | Fixed ESMBackbone device + sigmoid |
| `scripts/train_gflownet.py` | Added `--reward_type` and `--esm_model` flags |
| `docs/reward_formulation.md` | Added A/B/C reward options documentation |
| `docs/gflownet-master-prd.md` | Updated reward options table |
| `docs/prd-phase-3-training.md` | Added reward type selection + comparison runs |
| `docs/TODO-2025-12-26-1600.md` | Created + updated |

---

## W&B Dashboard

- Project: https://wandb.ai/ewijaya/gflownet-peptide
- Sanity check run: https://wandb.ai/ewijaya/gflownet-peptide/runs/5ej9lfs9
- Baseline 10k run: https://wandb.ai/ewijaya/gflownet-peptide/runs/8rflp7l6
- **Reward comparison runs** (in progress):
  - C (improved): `gflownet-reward-C-improved-10k`
  - B (esm2_pll): `gflownet-reward-B-esm2pll-10k`
  - A (trained): `gflownet-reward-A-trained-10k`

---

## Known Issues and Resolutions

### Issue 1: Flat Rewards from Untrained MLP Heads âœ… RESOLVED

**Problem**: The baseline training used `CompositeReward` with untrained MLP heads, producing near-constant rewards (~0.48-0.52). The log reward range was only 0.03 units vs policy log_PF of ~82 units - GFlowNet couldn't discriminate between sequences.

**Resolution**: Implemented `--reward_type` flag with three discriminative reward options:
- `improved` (Option C): Entropy gate + naturalness (~2-5 log units range)
- `esm2_pll` (Option B): ESM-2 pseudo-likelihood
- `trained` (Option A): Trained stability predictor

Currently running comparison (Step 2b) to validate which works best.

---

### Issue 2: TB Loss Scaling with log_Z

**Problem**: The Trajectory Balance loss increases as training progresses:

```
L_TB = (log Z + Î£ log P_F - log R - Î£ log P_B)Â²
```

As `log_Z` grows (2.9 â†’ 17.7), the squared term naturally scales up, causing raw loss to increase from 67 to 4079. This is mathematically expected but **not publishable** - reviewers expect decreasing loss curves.

**Status**: Secondary concern - first need to fix the flat reward issue (Issue 1).

**Potential Solutions** (to address after reward comparison):

1. **Normalized Loss Logging**: Add `loss / log_ZÂ²` metric
2. **Sub-Trajectory Balance (SubTB) Loss**: Already implemented, produces more stable curves
3. **Alternative Metrics for Paper**: Focus on reward distributions and diversity (standard in GFlowNet papers)

### References

- GFlowNet papers (Bengio et al.) typically show reward distributions and diversity rather than raw TB loss
- The "loss going up with log_Z" is a known issue in the GFlowNet community
