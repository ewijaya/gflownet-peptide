# GFlowNet Peptide - Session TODO

**Created**: 2025-12-26 16:00 JST
**Last Updated**: 2025-12-26 18:20 JST
**Previous TODO**: [TODO-2025-12-24-2333.md](./TODO-2025-12-24-2333.md)

---

## Current Phase: Phase 3 - Training & Hyperparameter Tuning

**Objective**: Train the GFlowNet model to convergence using Trajectory Balance loss, tune hyperparameters for optimal diversity-quality trade-off, and generate a large peptide dataset for downstream evaluation.

**PRD**: [docs/prd-phase-3-training.md](./prd-phase-3-training.md)

---

## Completed This Session

### Code Development

- Fixed `ESMBackbone` device handling for lazy loading (override `_apply()` method)
- Fixed `RewardHead` sigmoid transform (was returning raw values)
- Generated Phase 3 PRD from master PRD

### Training

- ✅ Sanity check (100 steps) completed successfully
- W&B run: https://wandb.ai/ewijaya/gflownet-peptide/runs/5ej9lfs9

### Tests

- All 123 tests passing
- Phase 2 coverage: 87%

---

## Phase 3 Command Execution Checklist

### Step 0: Environment Setup (Already Done)

```bash
# Verify GPU availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"

# Verify W&B login
wandb login --verify

# Create logs directory
mkdir -p logs checkpoints/gflownet samples outputs
```

- [x] GPU verified
- [x] W&B logged in
- [x] Directories created

---

### Step 1: Sanity Check (100 steps) ✅ COMPLETED

```bash
python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 100 \
  --batch_size 32 \
  --output_dir checkpoints/gflownet/sanity_check/ \
  --wandb
```

- [x] Training completed without errors
- [x] W&B metrics logged

---

### Step 2: Baseline Training (10K steps)

**Duration**: ~30-60 minutes

**Option A: Interactive (if staying connected)**
```bash
python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 10000 \
  --output_dir checkpoints/gflownet/baseline/ \
  --wandb
```

**Option B: Background with auto-shutdown (recommended)**
```bash
nohup bash -c 'python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 10000 \
  --output_dir checkpoints/gflownet/baseline/ \
  --run_name gflownet-baseline-10k \
  --wandb \
  > logs/train_baseline.log 2>&1 && /home/ubuntu/bin/stopinstance' &
```

**Monitoring**:
```bash
# Check progress
tail -f logs/train_baseline.log

# Check if running
ps aux | grep train_gflownet

# View W&B dashboard
# https://wandb.ai/ewijaya/gflownet-peptide
```

**Verification after completion**:
```bash
ls -la checkpoints/gflownet/baseline/

python -c "
import torch
ckpt = torch.load('checkpoints/gflownet/baseline/gflownet_final.pt')
print('Checkpoint keys:', ckpt.keys())
print('Step:', ckpt.get('step', 'N/A'))
"
```

- [x] Training in progress (61% at 18:15 JST)
- [x] Checkpoint saved (gflownet_latest.pt, gflownet_best.pt)
- [x] Metrics look reasonable (diversity excellent, no NaN)

**Current Status (18:15 JST)**:
- **Progress**: 6,112 / 10,000 steps (61%)
- **ETA**: ~1h remaining
- **W&B Run**: https://wandb.ai/ewijaya/gflownet-peptide/runs/8rflp7l6

| Step | Loss | Diversity | Mean Reward | log_Z |
|------|------|-----------|-------------|-------|
| 1000 | 204.52 | 0.479 | 0.444 | 2.93 |
| 2000 | 167.30 | 0.494 | 0.443 | 6.06 |
| 3000 | 109.12 | 0.486 | 0.443 | 8.67 |
| 4000 | 67.46 | 0.467 | 0.445 | 10.83 |
| 5000 | 599.26 | 0.669 | 0.443 | 13.26 |
| 6000 | 1972.14 | **0.821** | 0.451 | 15.56 |

**Key Observations**:
- ✅ Diversity excellent at 0.821 (target >0.4)
- ✅ Mean reward stable ~0.45 (no reward hacking)
- ⚠️ Loss increased from 67 → 1972 as log_Z grew (see issue below)

---

### Step 3: Analyze Baseline Results

```bash
# Check W&B for metrics
python -c "
import wandb
api = wandb.Api()
runs = api.runs('ewijaya/gflownet-peptide')
for run in runs[:5]:
    print(f'{run.name}: {run.state}, loss={run.summary.get(\"train/loss\", \"N/A\")}')"
```

- [ ] Loss converged or trending down
- [ ] log_Z stable (not diverging)
- [ ] No NaN values

---

### Step 4: Hyperparameter Sweep (Optional - Skip if baseline is good)

**Option A: W&B Sweep**
```bash
# Create sweep config
cat > configs/sweep.yaml << 'EOF'
program: scripts/train_gflownet.py
method: bayes
metric:
  name: train/loss
  goal: minimize
parameters:
  learning_rate:
    values: [1e-4, 3e-4, 1e-3]
  batch_size:
    values: [32, 64, 128]
EOF

# Create sweep
wandb sweep configs/sweep.yaml --project gflownet-peptide

# Run agent (replace <sweep_id> with actual ID)
# wandb agent ewijaya/gflownet-peptide/<sweep_id>
```

**Option B: Manual Grid Search**
```bash
# Learning rate sweep
for lr in 1e-4 3e-4 1e-3; do
  nohup python scripts/train_gflownet.py \
    --config configs/default.yaml \
    --n_steps 10000 \
    --output_dir checkpoints/gflownet/sweep_lr_${lr}/ \
    --wandb \
    > logs/sweep_lr_${lr}.log 2>&1 &
done
```

- [ ] Sweep completed (if running)
- [ ] Best hyperparameters identified
- [ ] `configs/best.yaml` created (if different from default)

---

### Step 5: Full Training (100K steps)

**Duration**: ~4-8 hours

```bash
nohup bash -c 'python scripts/train_gflownet.py \
  --config configs/default.yaml \
  --n_steps 100000 \
  --output_dir checkpoints/gflownet/final/ \
  --run_name gflownet-final-100k \
  --wandb \
  > logs/train_final.log 2>&1 && /home/ubuntu/bin/stopinstance' &
```

**Monitoring (from another session)**:
```bash
tail -f logs/train_final.log
```

**Verification after completion**:
```bash
ls -la checkpoints/gflownet/final/

python -c "
import torch
ckpt = torch.load('checkpoints/gflownet/final/gflownet_final.pt')
print('Step:', ckpt.get('step'))
print('Final log_Z:', ckpt.get('log_z'))
"
```

- [ ] Training completed to 100K steps
- [ ] Final checkpoint saved
- [ ] Loss converged
- [ ] log_Z stable

---

### Step 6: Generate Peptide Samples

```bash
python scripts/sample.py \
  --checkpoint checkpoints/gflownet/final/gflownet_final.pt \
  --n_samples 10000 \
  --output samples/gflownet_final.csv \
  --temperature 1.0
```

**Quick validation**:
```bash
python -c "
import pandas as pd
df = pd.read_csv('samples/gflownet_final.csv')
print(f'Total samples: {len(df)}')
print(f'Unique samples: {df[\"sequence\"].nunique()}')
print(f'Mean length: {df[\"sequence\"].str.len().mean():.1f}')
print(f'Mean reward: {df[\"reward\"].mean():.4f}')
"
```

- [ ] 10,000 samples generated
- [ ] Samples saved to CSV
- [ ] Basic stats look reasonable

---

### Step 7: Evaluate Samples

```bash
python scripts/evaluate.py \
  --gflownet_samples samples/gflownet_final.csv \
  --output outputs/phase3_eval.json
```

**View results**:
```bash
cat outputs/phase3_eval.json | python -m json.tool
```

- [ ] Evaluation completed
- [ ] Diversity metrics computed
- [ ] Quality metrics computed

---

### Step 8: Phase Gate Review

**Success Criteria Verification**:

| Criterion | Target | Actual | Status |
|-----------|--------|--------|--------|
| Training loss | Converged | ______ | [ ] |
| log_Z stability | No divergence | ______ | [ ] |
| Sample validity | ≥99% valid AA | ______ | [ ] |
| Sample diversity | > 0.5 | ______ | [ ] |
| Sample quality | Mean R > 0.5 | ______ | [ ] |
| Generated samples | 10,000 | ______ | [ ] |

**Final verification commands**:
```bash
# Check all outputs exist
ls -la checkpoints/gflownet/final/gflownet_final.pt
ls -la samples/gflownet_final.csv
ls -la outputs/phase3_eval.json
wc -l samples/gflownet_final.csv
```

- [ ] All success criteria met
- [ ] Ready for Phase 4 (Evaluation & GRPO Comparison)

---

## Quick Reference: All Commands in Order

```bash
# === STEP 1: Sanity Check (DONE) ===
python scripts/train_gflownet.py --config configs/default.yaml --n_steps 100 --batch_size 32 --output_dir checkpoints/gflownet/sanity_check/ --wandb

# === STEP 2: Baseline Training ===
mkdir -p logs
nohup bash -c 'python scripts/train_gflownet.py --config configs/default.yaml --n_steps 10000 --output_dir checkpoints/gflownet/baseline/ --run_name gflownet-baseline-10k --wandb > logs/train_baseline.log 2>&1 && /home/ubuntu/bin/stopinstance' &

# === STEP 5: Full Training (after baseline looks good) ===
nohup bash -c 'python scripts/train_gflownet.py --config configs/default.yaml --n_steps 100000 --output_dir checkpoints/gflownet/final/ --run_name gflownet-final-100k --wandb > logs/train_final.log 2>&1 && /home/ubuntu/bin/stopinstance' &

# === STEP 6: Generate Samples ===
python scripts/sample.py --checkpoint checkpoints/gflownet/final/gflownet_final.pt --n_samples 10000 --output samples/gflownet_final.csv --temperature 1.0

# === STEP 7: Evaluate ===
python scripts/evaluate.py --gflownet_samples samples/gflownet_final.csv --output outputs/phase3_eval.json
```

---

## Observations

- ESM-2 model (esm2_t33_650M) downloads on first use (~2.5GB)
- Training uses ~8GB GPU memory with batch_size=32
- Sanity check showed loss dropping from ~1049 to ~0.78 in 100 steps
- log_Z learning is working (started at 0, moved to 0.08)
- CompositeReward is functional after sigmoid fix

---

## Files Modified This Session

| File | Action |
|------|--------|
| `gflownet_peptide/models/reward_model.py` | Fixed ESMBackbone device + sigmoid |
| `docs/prd-phase-3-training.md` | Created + updated |
| `docs/TODO-2025-12-26-1600.md` | Created |

---

## W&B Dashboard

- Project: https://wandb.ai/ewijaya/gflownet-peptide
- Sanity check run: https://wandb.ai/ewijaya/gflownet-peptide/runs/5ej9lfs9
- Baseline 10k run: https://wandb.ai/ewijaya/gflownet-peptide/runs/8rflp7l6

---

## Known Issue: TB Loss Scaling with log_Z

### Problem

The Trajectory Balance loss increases as training progresses, even though the model is learning correctly:

```
L_TB = (log Z + Σ log P_F - log R - Σ log P_B)²
```

As `log_Z` grows (2.9 → 15.7), the squared term naturally scales up, causing raw loss to increase from 67 to 1972. This is mathematically expected but **not publishable** - reviewers expect decreasing loss curves.

### Current Baseline Results

Despite the loss curve issue, the model is working well:
- Diversity: 0.821 (excellent, target was >0.4)
- Mean reward: ~0.45 (stable, no reward hacking)
- log_Z: 15.7 (growing as expected)

### Solutions to Address for Publication

**Option 1: Normalized Loss Logging**
Add `loss / log_Z²` or `loss / log_Z` as a metric. This shows true convergence behavior.

```python
# In trainer.py, add:
normalized_loss = loss.item() / (log_z.item() ** 2 + 1e-8)
wandb.log({"train/normalized_loss": normalized_loss})
```

**Option 2: Sub-Trajectory Balance (SubTB) Loss**
Already implemented in `gflownet_peptide/training/losses.py`. SubTB averages over sub-trajectories, producing more stable loss curves.

```yaml
# In configs/default.yaml:
training:
  loss_type: "sub_trajectory_balance"  # instead of "trajectory_balance"
```

**Option 3: Alternative Metrics for Paper**
Focus on metrics that GFlowNet papers typically report:
- L1 distance between sampling and target distributions
- Reward-weighted negative log-likelihood
- Top-K reward over training
- Diversity metrics (already tracking)
- Mode coverage

**Option 4: Clamp log_Z Growth**
Limit log_Z growth rate to keep loss bounded:

```python
# Clamp log_Z to prevent unbounded growth
self.log_z.data.clamp_(max=10.0)
```

### Recommended Next Steps

1. ✅ Let current baseline (10k) finish - valuable for comparison
2. After completion, run a second baseline with SubTB loss
3. Add normalized loss logging to trainer
4. Compare loss curves between TB and SubTB for paper

### References

- GFlowNet papers (Bengio et al.) typically show reward distributions and diversity rather than raw TB loss
- The "loss going up with log_Z" is a known issue in the GFlowNet community
