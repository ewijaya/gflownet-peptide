# GFlowNet Peptide Project - TODO

**Created**: 2025-12-23 07:30 UTC
**Last Updated**: 2025-12-23 07:30 UTC

---

## In Progress

- [x] **GRPO-D Training Running** (started 2025-12-23 07:26 UTC)
  - 1000 iterations with `protgpt2-distilled-medium`
  - Monitor: https://wandb.ai/ewijaya/gflownet-peptide
  - Logs: `tail -f logs/train_grpo.log`

---

## Phase 0: Validation (Next Steps)

After training completes:

- [ ] Create analysis notebook `notebooks/gflownet-phase-0-validation.ipynb`
- [ ] Load generated peptides from `results/grpo/`
- [ ] Compute diversity metrics (sequence diversity, embedding diversity)
- [ ] Generate ESM-2 embeddings for clustering
- [ ] Run UMAP + HDBSCAN clustering
- [ ] Create visualization of clusters
- [ ] Generate random baseline for comparison
- [ ] Write go/no-go decision document

---

## How to Interpret W&B Results

Dashboard: https://wandb.ai/ewijaya/gflownet-peptide

### Key Metrics to Monitor

| Metric | What It Means | Healthy Range |
|--------|---------------|---------------|
| `mean_reward` | Average ESM-2 score across batch | Should increase over time |
| `max_reward` | Highest reward in batch | Watch for ceiling at 1.0 |
| `mean_diversity` | Diversity score (AA freq + Levenshtein) | 0.3 - 0.7 |
| `total_loss` | Combined policy + KL loss | Should decrease and stabilize |
| `policy_loss` | Policy gradient loss | Should decrease |
| `kl_loss` | KL divergence from reference model | Should stay bounded |

### Positive Signs (Working as Expected)

- ✅ `mean_reward` gradually increasing over iterations
- ✅ `mean_diversity` staying above 0.3-0.4
- ✅ `total_loss` decreasing and stabilizing
- ✅ Top peptides showing variety (different sequences, not all repeats)
- ✅ `kl_loss` staying bounded (not exploding)

### Negative Signs (Problems)

- ❌ `mean_diversity` dropping below 0.3 → **Mode collapse** (all sequences converging)
- ❌ `mean_reward` flat while `max_reward` is high → **Stuck in local optima**
- ❌ Top peptides all repetitive (`SSSS...`, `AAAA...`) → **Reward hacking**
- ❌ `kl_loss` exploding → **Policy diverging too far from reference**
- ❌ `total_loss` increasing or oscillating wildly → **Training instability**

### Expected Behavior for Phase 0

We **expect** to see some negative signs (reward hacking, low diversity) because:
- ESM-2 pseudo-likelihood rewards simple repeating patterns
- GRPO-D's diversity penalty (15%) may not be enough to prevent collapse
- This validates our hypothesis that GFlowNet is needed for true diversity

**If training shows high rewards but low diversity → GO decision for GFlowNet**
**If training shows high rewards AND high diversity → NO-GO (GRPO-D is sufficient)**

---

## Observations So Far

- ESM-2 pseudo-likelihood rewards simple repeats (`SSSSSS...`, `RARRAR...` → R=1.0)
- This confirms Phase 0 hypothesis: pure reward optimization → degenerate solutions
- Supports the need for GFlowNet's intrinsic diversity

---

## Files Created (Phase 0 Implementation)

| File | Purpose |
|------|---------|
| `gflownet_peptide/rewards/esm2_reward.py` | ESM-2 pseudo-likelihood reward |
| `gflownet_peptide/training/diversity.py` | Diversity calculation |
| `gflownet_peptide/models/grpo_policy.py` | PolicyValueNetwork |
| `gflownet_peptide/training/grpo_trainer.py` | GRPO-D trainer |
| `scripts/train_grpo.py` | Training CLI |
| `configs/grpo.yaml` | Hyperparameters |
| `docs/prd-phase-0-validation.md` | Phase 0 PRD |

---

## Future Phases

- **Phase 1**: Reward Model Development
- **Phase 2**: GFlowNet Core Implementation
- **Phase 3**: Training & Hyperparameter Tuning
- **Phase 4**: Evaluation & GRPO Comparison
- **Phase 5**: Documentation & Publication
